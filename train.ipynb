{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "statistical-rogers",
   "metadata": {},
   "source": [
    "# Example training notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crucial-anime",
   "metadata": {},
   "source": [
    "This notebook shows how to train a basic model on fixed scale images of the AIST Building Change Detection dataset. Feel free to modify it as you wish."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strange-commander",
   "metadata": {},
   "source": [
    "####  For Google Colab\n",
    "These cells are used to setup the repository, required packages and dataset in Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lovely-paradise",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    \n",
    "if IN_COLAB:\n",
    "    # Clone the entire repo to access the files\n",
    "    !git clone -l -s https://github.com/vita-epfl/introML-2021.git cloned-repo\n",
    "    # Go to the project directory\n",
    "    %cd cloned-repo/project/\n",
    "    # Install requirements\n",
    "    !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stretch-second",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell downloads the dataset if you're using Colab\n",
    "# If the dataset is already downloaded, avoid running it again after restarting the kernel\n",
    "if IN_COLAB:\n",
    "    !pip install gdown\n",
    "    !gdown https://drive.google.com/uc?id=1otKxIvEP77Cap9VmUkujMrAMo4K8_F1c\n",
    "    !unzip -q patch-pairs.zip -d data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conservative-kelly",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "administrative-bailey",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "environmental-salem",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import PatchPairsDataset, split_dataset\n",
    "from trainer import Trainer\n",
    "from evaluator import Evaluator\n",
    "from utils import show_pair, generate_submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loose-class",
   "metadata": {},
   "source": [
    "### Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "referenced-volume",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if device is cuda, then you are using a Nvidia GPU to train\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geographic-zambia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows extra GPU info if there is one\n",
    "if device.type == 'cuda':\n",
    "    !nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civic-telling",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polish-medicare",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More info about transforms: https://pytorch.org/vision/stable/transforms.html\n",
    "transform =  None \n",
    "batch_size = 16\n",
    "\n",
    "train_csv_path = \"data/train.csv\"\n",
    "test_csv_path = \"data/test.csv\"\n",
    "pairs_folder_path = \"data/patch-pairs\"\n",
    "\n",
    "train_data = PatchPairsDataset(csv_path=train_csv_path, pairs_folder_path=pairs_folder_path, transform=transform)\n",
    "\n",
    "# Split into train / val using split_dataset() from dataset.py\n",
    "train_data, val_data = split_dataset(train_data, split=0.2)\n",
    "\n",
    "test_data = PatchPairsDataset(csv_path=test_csv_path, pairs_folder_path=pairs_folder_path, transform=transform)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharp-report",
   "metadata": {},
   "outputs": [],
   "source": [
    "features, targets = iter(train_loader).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considerable-throw",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatal-jackson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first pair\n",
    "print(f\"Target: {targets[0].item():.0f}\")\n",
    "show_pair(features[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-assist",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expressed-indonesian",
   "metadata": {},
   "source": [
    "#### Network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latter-earth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try implementing new network architectures to improve your performance!\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    \"\"\"Baseline logistic regression model\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        # Input tensor is of shape [batch_size, 6, 160, 160]\n",
    "        self.fc = nn.Linear(6 * 160 * 160, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x.flatten(start_dim=1)\n",
    "        out = self.fc(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automatic-baking",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noble-promotion",
   "metadata": {},
   "source": [
    "#### Loss, optimizer and scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "orange-bones",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.BCEWithLogitsLoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=3e-4)\n",
    "scheduler = None \n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "determined-termination",
   "metadata": {},
   "source": [
    "### Saving, checkpoint and logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processed-genre",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to which the model weights will be saved\n",
    "timestr = time.strftime(\"%m%d-%H%M\")\n",
    "save_path = f\"outputs/{timestr}\"\n",
    "Path(save_path).mkdir(parents=True, exist_ok=True)\n",
    "save_path = os.path.join(save_path, \"\")\n",
    "# Resume training from checkpoint \n",
    "checkpoint_path = None\n",
    "# Log to tensorboard\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reduced-comment",
   "metadata": {},
   "source": [
    "#### TensorBoard within notebook \n",
    "It is possible to directly display a TensorBoard window within a notebook (instead of a separate browser tab). This is especially useful when using Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-country",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "actual-springer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If a timed out message is displayed, wait a bit and run this cell again.\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "right-extra",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "above-hindu",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch training\n",
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        loss_fn=loss_fn,\n",
    "        optimizer=optimizer,\n",
    "        epochs=epochs,\n",
    "        device=device,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        scheduler=scheduler,\n",
    "        writer=writer,\n",
    "        save_path=save_path,\n",
    "        checkpoint_path=checkpoint_path,\n",
    "        show_pbar=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incredible-chain",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noble-prague",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infrared-postage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the model that needs to be evaluated (can be None to use current model)\n",
    "eval_checkpoint_path = None\n",
    "\n",
    "evaluator = Evaluator(\n",
    "    model=model,\n",
    "    device=device,\n",
    "    loader=test_loader,\n",
    "    checkpoint_path=eval_checkpoint_path\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coral-reconstruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the data in evaluator has targets (e.g. train or val set), you can get the accuracy using evaluate()\n",
    "print(test_loader.dataset.has_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naval-bottom",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use predict() to get predictions (make sure the dataloader doesn't shuffle data)\n",
    "predictions = evaluator.predict(threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "headed-actress",
   "metadata": {},
   "source": [
    "### Generating a submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "labeled-dietary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use generate_submission() to get a correctly formatted CSV\n",
    "submission_path = \"outputs/submission.csv\"\n",
    "generate_submission(predictions, submission_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "metadata": {
   "interpreter": {
    "hash": "bf27883f24a5f59ded61cdabdf5884d91cb393caeb24728749abd210759b939b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
